{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9212b524-d2d1-4d94-9889-0fdc2b14c92c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_squared_log_error\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "def identify_best_column_with_data(df, target_station):\n",
    "    \"\"\"\n",
    "    Identifica la mejor columna que tiene la menor cantidad de valores nulos\n",
    "    en las filas donde la estación objetivo tiene valores faltantes.\n",
    "\n",
    "    Parameters:\n",
    "    - df: DataFrame que contiene las estaciones y los datos de las mismas.\n",
    "    - target_station: La estación que tiene valores faltantes y que se quiere completar.\n",
    "\n",
    "    Returns:\n",
    "    - best_column: Nombre de la columna con la menor cantidad de valores nulos en las filas donde la estación objetivo tiene valores faltantes.\n",
    "    \"\"\"\n",
    "    # Filtrar las filas donde la estación objetivo tiene valores NaN\n",
    "    target_nan_df = df[df[target_station].isna()]\n",
    "\n",
    "    # Verificar las columnas que tienen la menor cantidad de NaN en esas mismas filas\n",
    "    missing_counts = target_nan_df.drop(columns=[target_station]).isna().sum()\n",
    "\n",
    "    # Seleccionar la columna con la menor cantidad de NaN\n",
    "    best_column = missing_counts.idxmin()\n",
    "\n",
    "    return best_column\n",
    "\n",
    "def simple_regression(df, best_column, target_station):\n",
    "    # Filtrar las filas donde target_station tiene valores NaN\n",
    "    target_nan_df = df[df[target_station].isna()]\n",
    "\n",
    "    # Filtrar las filas donde best_column no tiene valores NaN (tiene datos)\n",
    "    target_nan_df_non_nan_other_columns_df = target_nan_df[target_nan_df[best_column].notna()]\n",
    "\n",
    "    # Filtrar las filas completas (sin NaN) en 'best_column' y 'target_station' para entrenar el modelo\n",
    "    remaining_df = df[['Fecha', best_column, target_station]].dropna()\n",
    "\n",
    "    # Verificar si hay suficientes datos para el train-test split\n",
    "    if len(remaining_df) < 2:\n",
    "        print(f\"No hay suficientes datos para realizar regresión en la estación: {target_station}\")\n",
    "        return df\n",
    "\n",
    "    # Realizar Train-test split (70% train, 30% test)\n",
    "    train_df, test_df = train_test_split(remaining_df, test_size=0.3, random_state=42)\n",
    "\n",
    "    # Inicializar el modelo de regresión lineal\n",
    "    linear_reg = LinearRegression()\n",
    "\n",
    "    # Entrenar el modelo\n",
    "    X_train = train_df[[best_column]]  # Usar solo 'best_column' como predictor\n",
    "    y_train = train_df[target_station]\n",
    "    linear_reg.fit(X_train, y_train)\n",
    "\n",
    "    # Usar el modelo entrenado para completar los valores faltantes en target_station\n",
    "    for index, row in target_nan_df_non_nan_other_columns_df.iterrows():\n",
    "        # Crear un DataFrame con el valor de 'best_column' para mantener los nombres de las características\n",
    "        X_pred_df = pd.DataFrame([[row[best_column]]], columns=[best_column])\n",
    "        predicted_value = linear_reg.predict(X_pred_df)[0]\n",
    "\n",
    "        # Verificar si el valor predicho es negativo o cercano a 0\n",
    "        if predicted_value < 0 or np.isclose(predicted_value, 0):\n",
    "            predicted_value = 0\n",
    "\n",
    "        # Rellenar el valor predicho en target_station\n",
    "        target_nan_df_non_nan_other_columns_df.loc[index, target_station] = np.round(predicted_value, 2)\n",
    "\n",
    "    # Fusionar los datos completados con el resto del DataFrame\n",
    "    merged_df = pd.concat([target_nan_df_non_nan_other_columns_df, remaining_df], ignore_index=True)\n",
    "    merged_df.sort_values(by='Fecha', inplace=True)\n",
    "    \n",
    "    return merged_df\n",
    "\n",
    "# Function to perform train-test split and complete missing data using regression\n",
    "def complete_missing_data(df, target_station):\n",
    "    # Drop rows with NaN values in the target station\n",
    "    # Create a copy of the dropped rows\n",
    "    dropped_df = df[df.isna().any(axis=1)]\n",
    "\n",
    "    # Filtrar las filas donde target_station es NaN\n",
    "    target_nan_df = df[df[target_station].isna()]\n",
    "\n",
    "    # Filtrar las filas donde el resto de las columnas no son NaN\n",
    "    target_nan_df_non_nan_other_columns_df = target_nan_df[target_nan_df.drop(columns=[target_station]).notna().all(axis=1)]\n",
    "\n",
    "    # Obtener las filas que están en dropped_df pero no en target_nan_df_non_nan_other_columns_df\n",
    "    rows_to_keep = dropped_df.index.difference(target_nan_df_non_nan_other_columns_df.index)\n",
    "    dropped_df_not_in_target_nan = dropped_df.loc[rows_to_keep]\n",
    "\n",
    "    # Drop the rows with NaN values in 'target_station' from the original DataFrame\n",
    "    remaining_df = df.dropna()\n",
    "    \n",
    "    # Train-test split (70% train, 30% test)\n",
    "    train_df, test_df = train_test_split(remaining_df, test_size=0.3, random_state=42)\n",
    "\n",
    "    # Initialize regression models\n",
    "    linear_reg = LinearRegression()\n",
    "    random_forest_reg = RandomForestRegressor()\n",
    "\n",
    "    # Define performance metric (RMSE in this example)\n",
    "    def root_mean_squared_error(y_true, y_pred):\n",
    "        return np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "\n",
    "    # Train models on the training set\n",
    "    X_train = train_df.drop([target_station, 'Fecha'], axis=1)\n",
    "    y_train = train_df[target_station]\n",
    "    linear_reg.fit(X_train, y_train)\n",
    "    random_forest_reg.fit(X_train, y_train)\n",
    "\n",
    "    # Evaluate models on the test set\n",
    "    X_test = test_df.drop([target_station, 'Fecha'], axis=1)\n",
    "    y_test = test_df[target_station]\n",
    "    linear_reg_rmse = root_mean_squared_error(y_test, linear_reg.predict(X_test))\n",
    "    random_forest_rmse = root_mean_squared_error(y_test, random_forest_reg.predict(X_test))\n",
    "\n",
    "    # Choose the best model based on performance (lower RMSE in this case)\n",
    "    if linear_reg_rmse < random_forest_rmse:\n",
    "        selected_model = linear_reg\n",
    "        selected_model_name = 'Linear Regression'\n",
    "        selected_model_rmse = linear_reg_rmse\n",
    "    else:\n",
    "        selected_model = random_forest_reg\n",
    "        selected_model_name = 'Random Forest'\n",
    "        selected_model_rmse = random_forest_rmse\n",
    "\n",
    "    # Use the selected model to complete missing values in the target station\n",
    "    feature_names = X_train.columns\n",
    "    for index, row in target_nan_df_non_nan_other_columns_df.iterrows():\n",
    "        if pd.isnull(row[target_station]):\n",
    "            X_pred = row.drop([target_station, 'Fecha']).values.reshape(1, -1)\n",
    "            X_pred_df = pd.DataFrame(X_pred, columns=feature_names)\n",
    "            predicted_value = selected_model.predict(X_pred_df)[0]\n",
    "\n",
    "            # Check if the predicted value is negative or close to 0\n",
    "            if predicted_value < 0 or np.isclose(predicted_value, 0):\n",
    "                predicted_value = 0\n",
    "                \n",
    "            target_nan_df_non_nan_other_columns_df.loc[index, target_station] = np.round(predicted_value,2)\n",
    "\n",
    "    # Merge them back together\n",
    "    merged_df = pd.concat([target_nan_df_non_nan_other_columns_df, remaining_df,dropped_df_not_in_target_nan], ignore_index=True)\n",
    "    merged_df.sort_values(by='Fecha', inplace=True)\n",
    "    return merged_df, selected_model_name, linear_reg_rmse,random_forest_rmse\n",
    "\n",
    "# Cálculo de NaN y completar hasta que no haya valores faltantes\n",
    "def complete_until_no_nan(df):\n",
    "    nan_percentages = df.isna().mean() * 100  # Porcentaje de valores NaN\n",
    "    ordered_columns = nan_percentages.sort_values().index.tolist()\n",
    "    ordered_columns.pop(0)  # Eliminar la columna de 'Fecha'\n",
    "    print(ordered_columns)\n",
    "\n",
    "    resultados = {\n",
    "        'Gauge': [],\n",
    "        'Selected Model': []\n",
    "    }\n",
    "\n",
    "    previous_nan_sum = nan_percentages.sum()  # Sumar NaN antes de empezar\n",
    "\n",
    "    # Iterar hasta que no queden valores faltantes\n",
    "    while nan_percentages.sum() > 0:\n",
    "        print(f\"Porcentajes de NaN: \\n{nan_percentages}\")\n",
    "        \n",
    "        for gauge in ordered_columns:\n",
    "            if df[gauge].isna().sum() > 0:  # Solo completar si la estación tiene valores faltantes\n",
    "                # Completación inicial usando el mejor método\n",
    "                df, model_name, linear_reg_rmse, random_forest_rmse = complete_missing_data(df, gauge)\n",
    "                resultados['Gauge'].append(gauge)\n",
    "                resultados['Selected Model'].append(model_name)\n",
    "                print(f\"Selected Model for {gauge}: {model_name}\")\n",
    "\n",
    "        # Recalcular los porcentajes de NaN después de completar los datos\n",
    "        nan_percentages = df.isna().mean() * 100\n",
    "        current_nan_sum = nan_percentages.sum()\n",
    "\n",
    "        # Si no mejora el porcentaje de NaN, hacer regresión simple\n",
    "        if current_nan_sum >= previous_nan_sum:\n",
    "            print(\"No hubo mejora en los valores faltantes, buscando la mejor columna para completar...\")\n",
    "            for gauge in ordered_columns:\n",
    "                if df[gauge].isna().sum() > 0:\n",
    "                    # Identificar la mejor columna con menos NaN para hacer la regresión\n",
    "                    best_column = identify_best_column_with_data(df.iloc[:, 1:], gauge)\n",
    "                    print(f\"Usando la columna {best_column} para completar {gauge}\")\n",
    "                    \n",
    "                    # Hacer regresión simple usando la mejor columna\n",
    "                    df = simple_regression(df, best_column, gauge)\n",
    "                    print(f\"Regresión simple completada para {gauge}\")\n",
    "\n",
    "        # Actualizamos el porcentaje de NaN para la siguiente iteración\n",
    "        previous_nan_sum = current_nan_sum\n",
    "\n",
    "    return df, pd.DataFrame(resultados)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "767c2688-6727-4cf4-b1eb-c7546fdcfe3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Fecha</th>\n",
       "      <th>chusis</th>\n",
       "      <th>chalaco</th>\n",
       "      <th>huamarca</th>\n",
       "      <th>huancabamba</th>\n",
       "      <th>miraflores</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1980-01-01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>2.1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1980-01-02</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1980-01-03</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1980-01-04</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1980-01-05</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Fecha  chusis  chalaco  huamarca  huancabamba  miraflores\n",
       "0  1980-01-01     0.0      0.0       0.4          2.1         0.0\n",
       "1  1980-01-02     0.0      0.0       0.0          0.0         0.0\n",
       "2  1980-01-03     0.0      0.0       0.0          1.5         0.0\n",
       "3  1980-01-04     0.0      0.0       0.0          0.0         0.0\n",
       "4  1980-01-05     0.0      0.0       0.0          0.0         0.0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs=pd.read_csv('../data/Estaciones/gauge.csv',sep=',')\n",
    "#obs['Fecha'] = pd.to_datetime(obs['Fecha'], dayfirst=True)\n",
    "#obs['Fecha'] = pd.to_datetime(obs['Fecha'])\n",
    "#obs.set_index('Fecha',inplace=True)\n",
    "obs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3153972b-27a6-4a3d-a795-f230f45093fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['huancabamba', 'huamarca', 'chalaco', 'miraflores', 'chusis']\n",
      "Porcentajes de NaN: \n",
      "Fecha           0.000000\n",
      "chusis         17.406340\n",
      "chalaco         4.178674\n",
      "huamarca        3.940922\n",
      "huancabamba     3.350144\n",
      "miraflores      7.536023\n",
      "dtype: float64\n",
      "Selected Model for huancabamba: Linear Regression\n",
      "Selected Model for huamarca: Linear Regression\n",
      "Selected Model for chalaco: Linear Regression\n",
      "Selected Model for miraflores: Linear Regression\n",
      "Selected Model for chusis: Linear Regression\n",
      "Porcentajes de NaN: \n",
      "Fecha          0.000000\n",
      "chusis         6.563401\n",
      "chalaco        1.988473\n",
      "huamarca       2.672911\n",
      "huancabamba    2.759366\n",
      "miraflores     5.036023\n",
      "dtype: float64\n",
      "Selected Model for huancabamba: Linear Regression\n",
      "Selected Model for huamarca: Linear Regression\n",
      "Selected Model for chalaco: Linear Regression\n",
      "Selected Model for miraflores: Random Forest\n",
      "Selected Model for chusis: Linear Regression\n",
      "No hubo mejora en los valores faltantes, buscando la mejor columna para completar...\n",
      "Usando la columna huamarca para completar huancabamba\n",
      "Regresión simple completada para huancabamba\n",
      "Usando la columna huamarca para completar chalaco\n",
      "Regresión simple completada para chalaco\n",
      "Usando la columna chalaco para completar miraflores\n",
      "Regresión simple completada para miraflores\n",
      "Usando la columna chalaco para completar chusis\n",
      "Regresión simple completada para chusis\n",
      "Porcentajes de NaN: \n",
      "Fecha          0.000000\n",
      "chusis         6.563401\n",
      "chalaco        1.988473\n",
      "huamarca       2.672911\n",
      "huancabamba    2.759366\n",
      "miraflores     5.036023\n",
      "dtype: float64\n",
      "Selected Model for huancabamba: Linear Regression\n",
      "Selected Model for huamarca: Linear Regression\n",
      "Selected Model for miraflores: Linear Regression\n",
      "Porcentajes de NaN: \n",
      "Fecha          0.000000\n",
      "chusis         0.000000\n",
      "chalaco        0.000000\n",
      "huamarca       1.635946\n",
      "huancabamba    1.635946\n",
      "miraflores     0.229477\n",
      "dtype: float64\n",
      "Selected Model for huancabamba: Random Forest\n",
      "Selected Model for huamarca: Linear Regression\n",
      "Selected Model for miraflores: Linear Regression\n",
      "No hubo mejora en los valores faltantes, buscando la mejor columna para completar...\n",
      "Usando la columna chusis para completar huancabamba\n",
      "Regresión simple completada para huancabamba\n",
      "Usando la columna chusis para completar huamarca\n",
      "No hay suficientes datos para realizar regresión en la estación: huamarca\n",
      "Regresión simple completada para huamarca\n",
      "Usando la columna chusis para completar chalaco\n",
      "Regresión simple completada para chalaco\n",
      "Usando la columna chusis para completar miraflores\n",
      "No hay suficientes datos para realizar regresión en la estación: miraflores\n",
      "Regresión simple completada para miraflores\n",
      "Porcentajes de NaN: \n",
      "Fecha          0.000000\n",
      "chusis         0.000000\n",
      "chalaco        0.000000\n",
      "huamarca       1.635946\n",
      "huancabamba    1.635946\n",
      "miraflores     0.229477\n",
      "dtype: float64\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "With n_samples=0, test_size=0.3 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Ejemplo de uso:\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m df_completado, resultados_df \u001b[38;5;241m=\u001b[39m complete_until_no_nan(obs)\n",
      "Cell \u001b[1;32mIn[1], line 172\u001b[0m, in \u001b[0;36mcomplete_until_no_nan\u001b[1;34m(df)\u001b[0m\n\u001b[0;32m    169\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m gauge \u001b[38;5;129;01min\u001b[39;00m ordered_columns:\n\u001b[0;32m    170\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m df[gauge]\u001b[38;5;241m.\u001b[39misna()\u001b[38;5;241m.\u001b[39msum() \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:  \u001b[38;5;66;03m# Solo completar si la estación tiene valores faltantes\u001b[39;00m\n\u001b[0;32m    171\u001b[0m         \u001b[38;5;66;03m# Completación inicial usando el mejor método\u001b[39;00m\n\u001b[1;32m--> 172\u001b[0m         df, model_name, linear_reg_rmse, random_forest_rmse \u001b[38;5;241m=\u001b[39m complete_missing_data(df, gauge)\n\u001b[0;32m    173\u001b[0m         resultados[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGauge\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(gauge)\n\u001b[0;32m    174\u001b[0m         resultados[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSelected Model\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(model_name)\n",
      "Cell \u001b[1;32mIn[1], line 100\u001b[0m, in \u001b[0;36mcomplete_missing_data\u001b[1;34m(df, target_station)\u001b[0m\n\u001b[0;32m     97\u001b[0m remaining_df \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mdropna()\n\u001b[0;32m     99\u001b[0m \u001b[38;5;66;03m# Train-test split (70% train, 30% test)\u001b[39;00m\n\u001b[1;32m--> 100\u001b[0m train_df, test_df \u001b[38;5;241m=\u001b[39m train_test_split(remaining_df, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.3\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[0;32m    102\u001b[0m \u001b[38;5;66;03m# Initialize regression models\u001b[39;00m\n\u001b[0;32m    103\u001b[0m linear_reg \u001b[38;5;241m=\u001b[39m LinearRegression()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\telemac\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:213\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    208\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    209\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    211\u001b[0m         )\n\u001b[0;32m    212\u001b[0m     ):\n\u001b[1;32m--> 213\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    217\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    220\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    221\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    222\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    223\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\telemac\\Lib\\site-packages\\sklearn\\model_selection\\_split.py:2778\u001b[0m, in \u001b[0;36mtrain_test_split\u001b[1;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[0;32m   2775\u001b[0m arrays \u001b[38;5;241m=\u001b[39m indexable(\u001b[38;5;241m*\u001b[39marrays)\n\u001b[0;32m   2777\u001b[0m n_samples \u001b[38;5;241m=\u001b[39m _num_samples(arrays[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m-> 2778\u001b[0m n_train, n_test \u001b[38;5;241m=\u001b[39m _validate_shuffle_split(\n\u001b[0;32m   2779\u001b[0m     n_samples, test_size, train_size, default_test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.25\u001b[39m\n\u001b[0;32m   2780\u001b[0m )\n\u001b[0;32m   2782\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m shuffle \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[0;32m   2783\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m stratify \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\telemac\\Lib\\site-packages\\sklearn\\model_selection\\_split.py:2408\u001b[0m, in \u001b[0;36m_validate_shuffle_split\u001b[1;34m(n_samples, test_size, train_size, default_test_size)\u001b[0m\n\u001b[0;32m   2405\u001b[0m n_train, n_test \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(n_train), \u001b[38;5;28mint\u001b[39m(n_test)\n\u001b[0;32m   2407\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_train \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m-> 2408\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   2409\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWith n_samples=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, test_size=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m and train_size=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2410\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresulting train set will be empty. Adjust any of the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2411\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maforementioned parameters.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(n_samples, test_size, train_size)\n\u001b[0;32m   2412\u001b[0m     )\n\u001b[0;32m   2414\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m n_train, n_test\n",
      "\u001b[1;31mValueError\u001b[0m: With n_samples=0, test_size=0.3 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters."
     ]
    }
   ],
   "source": [
    "# Ejemplo de uso:\n",
    "df_completado, resultados_df = complete_until_no_nan(obs)\n",
    "# print(resultados_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4827cfe3-5f6f-43e3-ab3b-6bfe8ae03db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "resultados_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50212e1b-fae8-4116-b35f-95af7111178a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#obs['Fecha'] = pd.to_datetime(obs['Fecha'])\n",
    "#obs.set_index('Fecha',inplace=True)\n",
    "#plt.figure(figsize=(15,10))\n",
    "#sns.heatmap(obs.isnull(), cbar=False)\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d502b8a7-3016-4552-95ea-0312c5d3e03f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_completado['Fecha'] = pd.to_datetime(df_completado['Fecha'])\n",
    "#df_completado.set_index('Fecha',inplace=True)\n",
    "#plt.figure(figsize=(15,10))\n",
    "#sns.heatmap(df_completado.isnull(), cbar=False)\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "adfbaa49-f723-46ec-a761-616a11318914",
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_best_column_with_data(df, target_station):\n",
    "    \"\"\"\n",
    "    Identifica la mejor columna que tiene la menor cantidad de valores nulos\n",
    "    en las filas donde la estación objetivo tiene valores faltantes.\n",
    "\n",
    "    Parameters:\n",
    "    - df: DataFrame que contiene las estaciones y los datos de las mismas.\n",
    "    - target_station: La estación que tiene valores faltantes y que se quiere completar.\n",
    "\n",
    "    Returns:\n",
    "    - best_column: Nombre de la columna con la menor cantidad de valores nulos en las filas donde la estación objetivo tiene valores faltantes.\n",
    "    \"\"\"\n",
    "    # Filtrar las filas donde la estación objetivo tiene valores NaN\n",
    "    target_nan_df = df[df[target_station].isna()]\n",
    "\n",
    "    # Verificar las columnas que tienen la menor cantidad de NaN en esas mismas filas\n",
    "    missing_counts = target_nan_df.drop(columns=[target_station]).isna().sum()\n",
    "\n",
    "    # Seleccionar la columna con la menor cantidad de NaN\n",
    "    best_column = missing_counts.idxmin()\n",
    "\n",
    "    return best_column\n",
    "\n",
    "def simple_regression(df, best_column, target_station,linear_reg = None):\n",
    "    # Filtrar las filas donde target_station tiene valores NaN\n",
    "    target_nan_df = df[df[target_station].isna()]\n",
    "\n",
    "    # Filtrar las filas donde best_column no tiene valores NaN (tiene datos)\n",
    "    target_nan_df_non_nan_other_columns_df = target_nan_df[target_nan_df[best_column].notna()]\n",
    "\n",
    "    if linear_reg == None:\n",
    "        # Filtrar las filas completas (sin NaN) en 'best_column' y 'target_station' para entrenar el modelo\n",
    "        remaining_df = df[['Fecha', best_column, target_station]].dropna()\n",
    "    \n",
    "        # Realizar Train-test split (70% train, 30% test)\n",
    "        train_df, test_df = train_test_split(remaining_df, test_size=0.3, random_state=42)\n",
    "    \n",
    "        # Inicializar el modelo de regresión lineal\n",
    "        linear_reg = LinearRegression()\n",
    "    \n",
    "        # Entrenar el modelo\n",
    "        X_train = train_df[[best_column]]  # Usar solo 'best_column' como predictor\n",
    "        y_train = train_df[target_station]\n",
    "        linear_reg.fit(X_train, y_train)\n",
    "\n",
    "    # Usar el modelo entrenado para completar los valores faltantes en target_station\n",
    "    for index, row in target_nan_df_non_nan_other_columns_df.iterrows():\n",
    "        # Crear un DataFrame con el valor de 'best_column' para mantener los nombres de las características\n",
    "        X_pred_df = pd.DataFrame([[row[best_column]]], columns=[best_column])\n",
    "        predicted_value = linear_reg.predict(X_pred_df)[0]\n",
    "\n",
    "        # Verificar si el valor predicho es negativo o cercano a 0\n",
    "        if predicted_value < 0 or np.isclose(predicted_value, 0):\n",
    "            predicted_value = 0\n",
    "\n",
    "        # Rellenar el valor predicho en target_station\n",
    "        target_nan_df_non_nan_other_columns_df.loc[index, target_station] = np.round(predicted_value, 2)\n",
    "\n",
    "    # Fusionar los datos completados con el resto del DataFrame\n",
    "    merged_df = pd.concat([target_nan_df_non_nan_other_columns_df, remaining_df], ignore_index=True)\n",
    "    merged_df.sort_values(by='Fecha', inplace=True)\n",
    "    \n",
    "    return merged_df,linear_reg\n",
    "# Function to perform train-test split and complete missing data using regression\n",
    "def complete_missing_data(df, target_station, selected_model=None):\n",
    "    # Drop rows with NaN values in the target station\n",
    "    # Create a copy of the dropped rows\n",
    "    dropped_df = df[df.isna().any(axis=1)]\n",
    "\n",
    "    # Filtrar las filas donde target_station es NaN\n",
    "    target_nan_df = df[df[target_station].isna()]\n",
    "\n",
    "    # Filtrar las filas donde el resto de las columnas no son NaN\n",
    "    target_nan_df_non_nan_other_columns_df = target_nan_df[target_nan_df.drop(columns=[target_station]).notna().all(axis=1)]\n",
    "\n",
    "    # Obtener las filas que están en dropped_df pero no en target_nan_df_non_nan_other_columns_df\n",
    "    rows_to_keep = dropped_df.index.difference(target_nan_df_non_nan_other_columns_df.index)\n",
    "    dropped_df_not_in_target_nan = dropped_df.loc[rows_to_keep]\n",
    "\n",
    "    if selected_model is None:\n",
    "        # Drop the rows with NaN values in 'target_station' from the original DataFrame\n",
    "        remaining_df = df.dropna()\n",
    "        \n",
    "        # Verificar si hay suficientes datos para el train-test split\n",
    "        if len(remaining_df) < 2:\n",
    "            print(f\"No hay suficientes datos para realizar regresión en la estación: {target_station}\")\n",
    "            return df, selected_model, None, None, None\n",
    "        \n",
    "        # Train-test split (70% train, 30% test)\n",
    "        train_df, test_df = train_test_split(remaining_df, test_size=0.3, random_state=42)\n",
    "    \n",
    "        # Initialize regression models\n",
    "        linear_reg = LinearRegression()\n",
    "        random_forest_reg = RandomForestRegressor()\n",
    "    \n",
    "        # Define performance metric (RMSE in this example)\n",
    "        def root_mean_squared_error(y_true, y_pred):\n",
    "            return np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    \n",
    "        # Train models on the training set\n",
    "        X_train = train_df.drop([target_station, 'Fecha'], axis=1)\n",
    "        y_train = train_df[target_station]\n",
    "        linear_reg.fit(X_train, y_train)\n",
    "        random_forest_reg.fit(X_train, y_train)\n",
    "\n",
    "        # Evaluate models on the test set\n",
    "        X_test = test_df.drop([target_station, 'Fecha'], axis=1)\n",
    "        y_test = test_df[target_station]\n",
    "        linear_reg_rmse = root_mean_squared_error(y_test, linear_reg.predict(X_test))\n",
    "        random_forest_rmse = root_mean_squared_error(y_test, random_forest_reg.predict(X_test))\n",
    "    \n",
    "        # Choose the best model based on performance (lower RMSE in this case)\n",
    "        if linear_reg_rmse < random_forest_rmse:\n",
    "            selected_model = linear_reg\n",
    "            selected_model_name = 'Linear Regression'\n",
    "            selected_model_rmse = linear_reg_rmse\n",
    "        else:\n",
    "            selected_model = random_forest_reg\n",
    "            selected_model_name = 'Random Forest'\n",
    "            selected_model_rmse = random_forest_rmse\n",
    "    else:\n",
    "        selected_model = selected_model\n",
    "        selected_model_rmse = None\n",
    "\n",
    "    # Use the selected model to complete missing values in the target station\n",
    "    feature_names = [col for col in df.columns if col not in [target_station, 'Fecha']]\n",
    "    for index, row in target_nan_df_non_nan_other_columns_df.iterrows():\n",
    "        if pd.isnull(row[target_station]):\n",
    "            X_pred = row.drop([target_station, 'Fecha']).values.reshape(1, -1)\n",
    "            X_pred_df = pd.DataFrame(X_pred, columns=feature_names)\n",
    "            predicted_value = selected_model.predict(X_pred_df)[0]\n",
    "\n",
    "            # Check if the predicted value is negative or close to 0\n",
    "            if predicted_value < 0 or np.isclose(predicted_value, 0):\n",
    "                predicted_value = 0\n",
    "                \n",
    "            target_nan_df_non_nan_other_columns_df.loc[index, target_station] = np.round(predicted_value, 2)\n",
    "\n",
    "    # Merge them back together\n",
    "    merged_df = pd.concat([target_nan_df_non_nan_other_columns_df, remaining_df, dropped_df_not_in_target_nan], ignore_index=True)\n",
    "    merged_df.sort_values(by='Fecha', inplace=True)\n",
    "    return merged_df, selected_model, linear_reg_rmse, random_forest_rmse, selected_model_name\n",
    "\n",
    "\n",
    "# Cálculo de NaN y completar hasta que no haya valores faltantes\n",
    "def complete_until_no_nan(df):\n",
    "    nan_percentages = df.isna().mean() * 100  # Porcentaje de valores NaN\n",
    "    ordered_columns = nan_percentages.sort_values().index.tolist()\n",
    "    ordered_columns.pop(0)  # Eliminar la columna de 'Fecha'\n",
    "    print(ordered_columns)\n",
    "\n",
    "    resultados = {\n",
    "        'Gauge': [],\n",
    "        'Selected Model': []\n",
    "    }\n",
    "\n",
    "    previous_nan_sum = nan_percentages.sum()  # Sumar NaN antes de empezar\n",
    "\n",
    "    model_rg = None\n",
    "    linear_reg = None\n",
    "    # Iterar hasta que no queden valores faltantes\n",
    "    while nan_percentages.sum() > 0:\n",
    "        print(f\"Porcentajes de NaN: \\n{nan_percentages}\")\n",
    "        \n",
    "        for gauge in ordered_columns:\n",
    "            if df[gauge].isna().sum() > 0:  # Solo completar si la estación tiene valores faltantes\n",
    "                # Completación inicial usando el mejor método\n",
    "                df, model_rg, linear_reg_rmse, random_forest_rmse,model_name = complete_missing_data(df, gauge,selected_model = model_rg)\n",
    "                print(model_rg)\n",
    "                resultados['Gauge'].append(gauge)\n",
    "                resultados['Selected Model'].append(model_name)\n",
    "                print(f\"Selected Model for {gauge}: {model_name}\")\n",
    "\n",
    "        # Recalcular los porcentajes de NaN después de completar los datos\n",
    "        nan_percentages = df.isna().mean() * 100\n",
    "        current_nan_sum = nan_percentages.sum()\n",
    "\n",
    "        # Si no mejora el porcentaje de NaN, hacer regresión simple\n",
    "        if current_nan_sum >= previous_nan_sum:\n",
    "            print(\"No hubo mejora en los valores faltantes, buscando la mejor columna para completar...\")\n",
    "            for gauge in ordered_columns:\n",
    "                if df[gauge].isna().sum() > 0:\n",
    "                    # Identificar la mejor columna con menos NaN para hacer la regresión\n",
    "                    best_column = identify_best_column_with_data(df.iloc[:, 1:], gauge)\n",
    "                    print(f\"Usando la columna {best_column} para completar {gauge}\")\n",
    "                    \n",
    "                    # Hacer regresión simple usando la mejor columna\n",
    "                    df, linear_reg = simple_regression(df, best_column, gauge,linear_reg = linear_reg)\n",
    "                    print(f\"Regresión simple completada para {gauge}\")\n",
    "\n",
    "        # Actualizamos el porcentaje de NaN para la siguiente iteración\n",
    "        previous_nan_sum = current_nan_sum\n",
    "\n",
    "    return df, pd.DataFrame(resultados)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a85224e1-e165-4cfe-8299-8eb187c45124",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['huancabamba', 'huamarca', 'chalaco', 'miraflores', 'chusis']\n",
      "Porcentajes de NaN: \n",
      "Fecha           0.000000\n",
      "chusis         17.406340\n",
      "chalaco         4.178674\n",
      "huamarca        3.940922\n",
      "huancabamba     3.350144\n",
      "miraflores      7.536023\n",
      "dtype: float64\n",
      "LinearRegression()\n",
      "Selected Model for huancabamba: Linear Regression\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The feature names should match those that were passed during fit.\nFeature names unseen at fit time:\n- huancabamba\nFeature names seen at fit time, yet now missing:\n- huamarca\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Ejemplo de uso:\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m df_completado, resultados_df \u001b[38;5;241m=\u001b[39m complete_until_no_nan(obs)\n",
      "Cell \u001b[1;32mIn[22], line 168\u001b[0m, in \u001b[0;36mcomplete_until_no_nan\u001b[1;34m(df)\u001b[0m\n\u001b[0;32m    165\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m gauge \u001b[38;5;129;01min\u001b[39;00m ordered_columns:\n\u001b[0;32m    166\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m df[gauge]\u001b[38;5;241m.\u001b[39misna()\u001b[38;5;241m.\u001b[39msum() \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:  \u001b[38;5;66;03m# Solo completar si la estación tiene valores faltantes\u001b[39;00m\n\u001b[0;32m    167\u001b[0m         \u001b[38;5;66;03m# Completación inicial usando el mejor método\u001b[39;00m\n\u001b[1;32m--> 168\u001b[0m         df, model_rg, linear_reg_rmse, random_forest_rmse,model_name \u001b[38;5;241m=\u001b[39m complete_missing_data(df, gauge,selected_model \u001b[38;5;241m=\u001b[39m model_rg)\n\u001b[0;32m    169\u001b[0m         \u001b[38;5;28mprint\u001b[39m(model_rg)\n\u001b[0;32m    170\u001b[0m         resultados[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGauge\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(gauge)\n",
      "Cell \u001b[1;32mIn[22], line 131\u001b[0m, in \u001b[0;36mcomplete_missing_data\u001b[1;34m(df, target_station, selected_model)\u001b[0m\n\u001b[0;32m    129\u001b[0m X_pred \u001b[38;5;241m=\u001b[39m row\u001b[38;5;241m.\u001b[39mdrop([target_station, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFecha\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;241m.\u001b[39mvalues\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    130\u001b[0m X_pred_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(X_pred, columns\u001b[38;5;241m=\u001b[39mfeature_names)\n\u001b[1;32m--> 131\u001b[0m predicted_value \u001b[38;5;241m=\u001b[39m selected_model\u001b[38;5;241m.\u001b[39mpredict(X_pred_df)[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    133\u001b[0m \u001b[38;5;66;03m# Check if the predicted value is negative or close to 0\u001b[39;00m\n\u001b[0;32m    134\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m predicted_value \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m np\u001b[38;5;241m.\u001b[39misclose(predicted_value, \u001b[38;5;241m0\u001b[39m):\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\telemac\\Lib\\site-packages\\sklearn\\linear_model\\_base.py:306\u001b[0m, in \u001b[0;36mLinearModel.predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    292\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[0;32m    293\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    294\u001b[0m \u001b[38;5;124;03m    Predict using the linear model.\u001b[39;00m\n\u001b[0;32m    295\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    304\u001b[0m \u001b[38;5;124;03m        Returns predicted values.\u001b[39;00m\n\u001b[0;32m    305\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 306\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decision_function(X)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\telemac\\Lib\\site-packages\\sklearn\\linear_model\\_base.py:285\u001b[0m, in \u001b[0;36mLinearModel._decision_function\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    282\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_decision_function\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[0;32m    283\u001b[0m     check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m--> 285\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_data(X, accept_sparse\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsr\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsc\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcoo\u001b[39m\u001b[38;5;124m\"\u001b[39m], reset\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    286\u001b[0m     coef_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcoef_\n\u001b[0;32m    287\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m coef_\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\telemac\\Lib\\site-packages\\sklearn\\base.py:608\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[0;32m    537\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_validate_data\u001b[39m(\n\u001b[0;32m    538\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    539\u001b[0m     X\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mno_validation\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    544\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params,\n\u001b[0;32m    545\u001b[0m ):\n\u001b[0;32m    546\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Validate input data and set or check the `n_features_in_` attribute.\u001b[39;00m\n\u001b[0;32m    547\u001b[0m \n\u001b[0;32m    548\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    606\u001b[0m \u001b[38;5;124;03m        validated.\u001b[39;00m\n\u001b[0;32m    607\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 608\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_feature_names(X, reset\u001b[38;5;241m=\u001b[39mreset)\n\u001b[0;32m    610\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_tags()[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequires_y\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m    611\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    612\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m estimator \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    613\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequires y to be passed, but the target y is None.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    614\u001b[0m         )\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\telemac\\Lib\\site-packages\\sklearn\\base.py:535\u001b[0m, in \u001b[0;36mBaseEstimator._check_feature_names\u001b[1;34m(self, X, reset)\u001b[0m\n\u001b[0;32m    530\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m missing_names \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m unexpected_names:\n\u001b[0;32m    531\u001b[0m     message \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    532\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFeature names must be in the same order as they were in fit.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    533\u001b[0m     )\n\u001b[1;32m--> 535\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(message)\n",
      "\u001b[1;31mValueError\u001b[0m: The feature names should match those that were passed during fit.\nFeature names unseen at fit time:\n- huancabamba\nFeature names seen at fit time, yet now missing:\n- huamarca\n"
     ]
    }
   ],
   "source": [
    "# Ejemplo de uso:\n",
    "df_completado, resultados_df = complete_until_no_nan(obs)\n",
    "# print(resultados_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c1c28d-aad6-4557-a0d3-676b81cce3f4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
