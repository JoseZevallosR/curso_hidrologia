#Maskshape: Shape form of the final results
n=dim(rain_stats)[1]
#Maximum and minimum search parameters space
Lmin=matrix(c(1,0.001,0.001,0.001,0.0854,1),nrow = 6,ncol = n)
Lmax=matrix(c(4,0.1,0.1,0.1,0.1,20),nrow=6,ncol=n)
#print('Calculating the initial parameters ...')
#Initial parameters
parameters0=matrix(data=NA,nrow =n,ncol = 6)
n.cores <- parallel::detectCores() - 1
#create the cluster
my.cluster <- parallel::makeCluster(
n.cores,
type = "PSOCK"
)
#register it to be used by %dopar%
doParallel::registerDoParallel(cl = my.cluster)
parameters0=t(matrix(foreach(
i=1:n,
.combine = 'c',
.packages = "HyetosMinute"
) %dopar% {
momentos=rain_stats[i,]
mean24 = momentos$mean24
var24 = momentos$var24
cov24lag1 =momentos$autocov24
pdr24=momentos$dryperiod24
var3=momentos$var3
var6=momentos$var6
var12=momentos$var12
var18=momentos$var18
par=fun(mean24,var24,cov24lag1,pdr24,var3,var6,var12,var18,Lmin[,i],Lmax[,i])
return(par)
},nrow = 6,ncol = n))
parallel::stopCluster(cl = my.cluster) #closing the cluster
parameters=cbind(rain_stats[,1:2],parameters0)
names(parameters)=c('x','y','a','l','v','k','f','mx')
print('Reptitive Cross Validations ...')
#CV_parameters=repetitiveCV(times = iterations,parameters,rain_stats,Lmin = Lmin ,Lmax = Lmax)
#saving the initial parameters
#write.table(CV_parameters,paste0(path,'parameters01.csv'),sep = ',',row.names = F)
#CV_parameters
parameters
}
run(rain_stats,path,1)
#Importing the regionalization functionality
source('D:/Proyectos_GitHub/Bartlet-Lewis_Regionalization/code/RMBLRP.R')
#Importing the regionalization functionality
source('D:/Proyectos_GitHub/Bartlet-Lewis_Regionalization/code/RMBLRP.R')
run=function(rain_stats,path,iterations=5,fun=MBLRPM){
#rain_stats: contains the rainfall statistics
#path: where to save the results
#Maskshape: Shape form of the final results
n=dim(rain_stats)[1]
#Maximum and minimum search parameters space
Lmin=matrix(c(1,0.001,0.001,0.001,0.0854,1),nrow = 6,ncol = n)
Lmax=matrix(c(4,0.1,0.1,0.1,0.1,20),nrow=6,ncol=n)
#print('Calculating the initial parameters ...')
#Initial parameters
parameters0=matrix(data=NA,nrow =n,ncol = 6)
n.cores <- parallel::detectCores() - 1
#create the cluster
my.cluster <- parallel::makeCluster(
n.cores,
type = "PSOCK"
)
#register it to be used by %dopar%
doParallel::registerDoParallel(cl = my.cluster)
parameters0=t(matrix(foreach(
i=1:n,
.combine = 'c',
.packages = "HyetosMinute"
) %dopar% {
momentos=rain_stats[i,]
mean24 = momentos$mean24
var24 = momentos$var24
cov24lag1 =momentos$autocov24
pdr24=momentos$dryperiod24
var3=momentos$var3
var6=momentos$var6
var12=momentos$var12
var18=momentos$var18
par=fun(mean24,var24,cov24lag1,pdr24,var3,var6,var12,var18,Lmin[,i],Lmax[,i])
return(par)
},nrow = 6,ncol = n))
parallel::stopCluster(cl = my.cluster) #closing the cluster
parameters=cbind(rain_stats[,1:2],parameters0)
names(parameters)=c('x','y','a','l','v','k','f','mx')
print('Reptitive Cross Validations ...')
#CV_parameters=repetitiveCV(times = iterations,parameters,rain_stats,Lmin = Lmin ,Lmax = Lmax)
#saving the initial parameters
#write.table(CV_parameters,paste0(path,'parameters01.csv'),sep = ',',row.names = F)
#CV_parameters
parameters
}
run(rain_stats,path,1)
#Importing the regionalization functionality
source('D:/Proyectos_GitHub/Bartlet-Lewis_Regionalization/code/RMBLRP.R')
#Importing the regionalization functionality
source('D:/Proyectos_GitHub/Bartlet-Lewis_Regionalization/code/RMBLRP.R')
#Mixed stats from gauge stations and corrected TRMM
gauge_stats=read.csv('D:/Proyectos_GitHub/Bartlet-Lewis_Regionalization/data/gauge_stats.csv')
gauge_stats=kickOutliers(gauge_stats)
gauge_stats=filter_Neigbors(gauge_stats)
n=dim(gauge_stats)[1]
#validation_parameters=matrix(data=NA,nrow=n,ncol=6)
validation_parameters=read.csv('D:/Proyectos_GitHub/Bartlet-Lewis_Regionalization/output/CV_validation/CrossValidationParameters.csv')
gauge_help=gauge_stats
coordinates(gauge_help) <- ~x+y
proj4string(gauge_help)='+proj=longlat +datum=WGS84 +no_defs +ellps=WGS84 +towgs84=0,0,0'
mdist=distm(gauge_help)
try(for (i in 100:n){
stats=gauge_stats[-i,]
station=gauge_stats[i,]
print(paste('Cross Validation: ',as.character(i)))
CV_parameters=try(run(stats,path="D:/Proyectos_GitHub/Bartlet-Lewis_Regionalization/output/CV_validation/",iterations=5))
for (j in 1:6){
info=CV_parameters[[c('a','l','v','k','f','mx')[j]]]
denominador=sum((1/mdist[i,-i])^2)
validation_parameters[i,j]=sum(info/mdist[i,-i]^2/denominador)
}
write.table(validation_parameters,'D:/Proyectos_GitHub/Bartlet-Lewis_Regionalization/output/CV_validation/CrossValidationParameters.csv',sep=',',row.names = F)
})
#validation_parameters=matrix(data=NA,nrow=n,ncol=6)
validation_parameters=read.csv('D:/Proyectos_GitHub/Bartlet-Lewis_Regionalization/output/CV_validation/CrossValidationParameters.csv')
validation_parameters
try(for (i in 137:n){
stats=gauge_stats[-i,]
station=gauge_stats[i,]
print(paste('Cross Validation: ',as.character(i)))
CV_parameters=try(run(stats,path="D:/Proyectos_GitHub/Bartlet-Lewis_Regionalization/output/CV_validation/",iterations=5))
for (j in 1:6){
info=CV_parameters[[c('a','l','v','k','f','mx')[j]]]
denominador=sum((1/mdist[i,-i])^2)
validation_parameters[i,j]=sum(info/mdist[i,-i]^2/denominador)
}
write.table(validation_parameters,'D:/Proyectos_GitHub/Bartlet-Lewis_Regionalization/output/CV_validation/CrossValidationParameters.csv',sep=',',row.names = F)
})
try(for (i in 138:n){
stats=gauge_stats[-i,]
station=gauge_stats[i,]
print(paste('Cross Validation: ',as.character(i)))
CV_parameters=try(run(stats,path="D:/Proyectos_GitHub/Bartlet-Lewis_Regionalization/output/CV_validation/",iterations=5))
for (j in 1:6){
info=CV_parameters[[c('a','l','v','k','f','mx')[j]]]
denominador=sum((1/mdist[i,-i])^2)
validation_parameters[i,j]=sum(info/mdist[i,-i]^2/denominador)
}
write.table(validation_parameters,'D:/Proyectos_GitHub/Bartlet-Lewis_Regionalization/output/CV_validation/CrossValidationParameters.csv',sep=',',row.names = F)
})
validation_parameters
i=78
stats=gauge_stats[-i,]
station=gauge_stats[i,]
print(paste('Cross Validation: ',as.character(i)))
CV_parameters=try(run(stats,path="D:/Proyectos_GitHub/Bartlet-Lewis_Regionalization/output/CV_validation/",iterations=5))
for (j in 1:6){
info=CV_parameters[[c('a','l','v','k','f','mx')[j]]]
denominador=sum((1/mdist[i,-i])^2)
validation_parameters[i,j]=sum(info/mdist[i,-i]^2/denominador)
}
station
i=137
stats=gauge_stats[-i,]
station=gauge_stats[i,]
station
i=1
station=gauge_stats[i,]
station
gauge_stats
#Importing the regionalization functionality
source('D:/Proyectos_GitHub/Bartlet-Lewis_Regionalization/code/RMBLRP.R')
#Mixed stats from gauge stations and corrected TRMM
gauge_stats=read.csv('D:/Proyectos_GitHub/Bartlet-Lewis_Regionalization/data/gauge_stats.csv')
gauge_stats=kickOutliers(gauge_stats)
gauge_stats=filter_Neigbors(gauge_stats)
n=dim(gauge_stats)[1]
n
gauge_help=gauge_stats
coordinates(gauge_help) <- ~x+y
proj4string(gauge_help)='+proj=longlat +datum=WGS84 +no_defs +ellps=WGS84 +towgs84=0,0,0'
mdist=distm(gauge_help)
try(for (i in 1:n){
stats=gauge_stats[-i,]
station=gauge_stats[i,]
print(paste('Cross Validation: ',as.character(i)))
CV_parameters=try(run(stats,path="D:/Proyectos_GitHub/Bartlet-Lewis_Regionalization/output/CV_validation/",iterations=25))
for (j in 1:6){
info=CV_parameters[[c('a','l','v','k','f','mx')[j]]]
denominador=sum((1/mdist[i,-i])^2)
validation_parameters[i,j]=sum(info/mdist[i,-i]^2/denominador)
}
write.table(validation_parameters,'D:/Proyectos_GitHub/Bartlet-Lewis_Regionalization/output/CV_validation/CrossValidationParameters2.csv',sep=',',row.names = F)
})
library(HyetosMinute)
library(HyetosMinute)
####################################
###Model Statistics#################
####################################
#Mean
meanMBLRPM<-function(a,l,v,k,f,mx,h=1) {
x<-(h*l*mx*v*(1+k/f))/(a-1)
return(x)
}
#Variance
varMBLRPM<-function(a,l,v,k,f,mx,h=1) {
A<-(2*l*(1+k/f)*(mx^2)*(v^a))/((f^2)*((f^2)-1)*(a-1)*(a-2)*(a-3))
B<-(2*(f^2)-2+k*f)*(f^2)*((a-3)*h*(v^(2-a))-(v^(3-a))+((v+h)^(3-a)))
C<-k*(f*(a-3)*h*(v^(2-a))-(v^(3-a))+((v+f*h)^(3-a)))
D<-A*(B-C)
return(D)
}
#Covariance
covarMBLRPM<-function(a,l,v,k,f,mx,h=1,lag=1) {
A<-(l*(1+k/f)*(mx^2)*(v^a))/((f^2)*((f^2)-1)*(a-1)*(a-2)*(a-3))
B<-(2*(f^2)-2+k*f)*(f^2)*(((v+(lag+1)*h)^(3-a))-2*((v+lag*h)^(3-a))+((v+(lag-1)*h)^(3-a)))
C<-k*(((v+(lag+1)*h*f)^(3-a))-(2*((v+h*lag*f)^(3-a)))+((v+(lag-1)*h*f)^(3-a)))
D<-A*(B-C)
return(D)
}
#Dry probabilities
pdrMBLRPM<-function(a,l,v,k,f,h=1) {
mt<-((1+(f*(k+f))-(0.25*f*(k+f)*(k+4*f))+((f/72)*(k+f)*(4*(k^2)+27*k*f+72*(f^2))))*v)/(f*(a-1))
G00<-((1-k-f+1.5*k*f+(f^2)+0.5*(k^2))*v)/(f*(a-1))
A<-(f+(k*(v/(v+(k+f)*h))^(a-1)))/(f+k)
D<-exp(l*(-h-mt+G00*A))
return(D)
}
validation_parameters=read.csv('D:/Proyectos_GitHub/Bartlet-Lewis_Regionalization/output/CV_validation/CrossValidationParameters.csv')
validation_parameters
help("as.Date")
as.Date('2021-08-08')
months(as.Date('2021-08-01'))
as.Date('2021-08-08')
as.Date('2021-08-08')+7
as.Date('2021-08-10')+7
for (week in 0:17){
print(paste("Module",as.character(week+1),as.Date('2021-08-10')+7*week))
}
for (week in 0:17){
print(paste("Module",as.character(week+1),as.Date('2021-08-11')+7*week))
}
as.Date('2021-08-10')+7
for (week in 0:17){
print(paste("Module",as.character(week+1),as.Date('2021-08-11')+7*week))
}
for (week in 0:17){
print(paste("Module",as.character(week+1),as.Date('2021-08-10')+7*week))
}
for (week in 0:17){
print(paste("Module",as.character(week+1),as.Date('2021-08-14')+7*week))
}
for (week in 0:17){
print(paste("Module",as.character(week+1),as.Date('2021-08-9')+7*week))
}
acos(cos(2))
acos(1)
acos(1.2/2)
2*acos(1.2/2)
files=list.files("D:/Senamhi_consultoria_2021_2/datos/CHIRILU_IMERGE",
full.names = TRUE)
files
length(files)
tail(files)
grepl('2014', ' el 2014 sucks')
help(grepl)
grepl('2014', ' el 2014 sucks',fixed=T)
grepl('2014',file,fixed=T)
grepl('2014',files[1:10],fixed=T)
grepl('2014',files[10000,],fixed=T)
grepl('2014',files[10000:10100],fixed=T)
files[10000:10100]
length(files)
ano=grepl('2014',files,fixed=T)
sum(ano)
files[ano]
tail(files[ano])
length(files[ano])
files=list.files("D:/Senamhi_consultoria_2021_2/datos/CHIRILU_IMERGE",
full.names = TRUE)
ano=grepl('2014',files,fixed=T)
files[ano]
length(files[ano])
library(raster)
files=list.files("D:/Senamhi_consultoria_2021_2/datos/CHIRILU_IMERGE",
full.names = TRUE)
ano=grepl('2014',files,fixed=T)
list_raster <- files[ano] %>%
lapply(function(z){
raster::raster(z)
})
rm(list = ls())
"%>%" = magrittr::`%>%`
library(raster)
files=list.files("D:/Senamhi_consultoria_2021_2/datos/CHIRILU_IMERGE",
full.names = TRUE)
ano=grepl('2014',files,fixed=T)
list_raster <- files[ano] %>%
lapply(function(z){
raster::raster(z)
})
list_raster <- mapply(function(x, y){
list_raster[[x]] + list_raster[[y]]
},
x = seq(1, length(list_raster), 2),
y = seq(2, length(list_raster), 2))
length(list_raster)
# # early hourly stack
raster::writeRaster(list_raster,
filename = "D:/Senamhi_consultoria_2021_2/datos/early_hourly/early_chirilu2014.nc")
# # early hourly stack
raster::writeRaster(stack(list_raster),
filename = "D:/Senamhi_consultoria_2021_2/datos/early_hourly/early_chirilu2014.nc")
ano=grepl('2015',files,fixed=T)
list_raster <- files[ano] %>%
lapply(function(z){
raster::raster(z)
})
list_raster <- mapply(function(x, y){
list_raster[[x]] + list_raster[[y]]
},
x = seq(1, length(list_raster), 2),
y = seq(2, length(list_raster), 2))
# # early hourly stack
raster::writeRaster(stack(list_raster),
filename = "D:/Senamhi_consultoria_2021_2/datos/early_hourly/early_chirilu2015.nc")
length(files[ano])
ano=grepl('2016',files,fixed=T)
length(files[ano])
list_raster <- files[ano] %>%
lapply(function(z){
raster::raster(z)
})
# # early hourly stack
raster::writeRaster(stack(list_raster),
filename = "D:/Senamhi_consultoria_2021_2/datos/early_hourly/early_chirilu2016.nc")
list_raster <- mapply(function(x, y){
list_raster[[x]] + list_raster[[y]]
},
x = seq(1, length(list_raster), 2),
y = seq(2, length(list_raster), 2))
ano=grepl('2017',files,fixed=T)
length(files[ano])
list_raster <- files[ano] %>%
lapply(function(z){
raster::raster(z)
})
# # early hourly stack
raster::writeRaster(stack(list_raster),
filename = "D:/Senamhi_consultoria_2021_2/datos/early_hourly/early_chirilu2017.nc")
list_raster <- mapply(function(x, y){
list_raster[[x]] + list_raster[[y]]
},
x = seq(1, length(list_raster), 2),
y = seq(2, length(list_raster), 2))
ano=grepl('2018',files,fixed=T)
list_raster <- files[ano] %>%
lapply(function(z){
raster::raster(z)
})
# # early hourly stack
raster::writeRaster(stack(list_raster),
filename = "D:/Senamhi_consultoria_2021_2/datos/early_hourly/early_chirilu2018.nc")
list_raster <- mapply(function(x, y){
list_raster[[x]] + list_raster[[y]]
},
x = seq(1, length(list_raster), 2),
y = seq(2, length(list_raster), 2))
files=list.files("D:/Senamhi_consultoria_2021_2/datos/CHIRILU_IMERGE",
full.names = TRUE)
ano=grepl('2019',files,fixed=T)
list_raster <- files[ano] %>%
lapply(function(z){
raster::raster(z)
})
# # early hourly stack
raster::writeRaster(stack(list_raster),
filename = "D:/Senamhi_consultoria_2021_2/datos/early_hourly/early_chirilu2019.nc")
list_raster <- mapply(function(x, y){
list_raster[[x]] + list_raster[[y]]
},
x = seq(1, length(list_raster), 2),
y = seq(2, length(list_raster), 2))
ano=grepl('2020',files,fixed=T)
list_raster <- files[ano] %>%
lapply(function(z){
raster::raster(z)
})
# # early hourly stack
raster::writeRaster(stack(list_raster),
filename = "D:/Senamhi_consultoria_2021_2/datos/early_hourly/early_chirilu2020.nc")
list_raster <- mapply(function(x, y){
list_raster[[x]] + list_raster[[y]]
},
x = seq(1, length(list_raster), 2),
y = seq(2, length(list_raster), 2))
length(files[ano])
ano=grepl('2020',files,fixed=T)
ano
ano=grepl('2020',files,fixed=T)
list_raster <- files[ano] %>%
lapply(function(z){
raster::raster(z)
})
# # early hourly stack
raster::writeRaster(stack(list_raster),
filename = "D:/Senamhi_consultoria_2021_2/datos/early_hourly/early_chirilu2020.nc")
list_raster <- mapply(function(x, y){
list_raster[[x]] + list_raster[[y]]
},
x = seq(1, length(list_raster), 2),
y = seq(2, length(list_raster), 2))
ano=grepl('2021',files,fixed=T)
length(files[ano])
ano
ano=grepl('2020',files,fixed=T)
ano
plot(stack(list_raster[1:12]))
files
ano=grepl('.2021',files,fixed=T)
ano
length(files[ano])
(length(files[ano])-1)/2
files[ano][-length(files[ano])]
length(files[ano][-length(files[ano])])
ano=grepl('.2021',files,fixed=T)
list_raster <- files[ano][-length(files[ano])] %>%
lapply(function(z){
raster::raster(z)
})
# # early hourly stack
raster::writeRaster(stack(list_raster),
filename = "D:/Senamhi_consultoria_2021_2/datos/early_hourly/early_chirilu2021.nc")
list_raster <- mapply(function(x, y){
list_raster[[x]] + list_raster[[y]]
},
x = seq(1, length(list_raster), 2),
y = seq(2, length(list_raster), 2))
setwd("D:/Senamhi_consultoria_2021_2/datos/early_hourly/")
mapa=stack('early_chirilu2014.nc')
plot(mapa[1:10])
plot(mapa[[1:10]])
plot(mapa[[1:40]])
#https://theconstructor.org/water-resources/analysis-presentation-of-rainfall-data/4493/
#http://www.di.fc.ul.pt/~jpn/r/distributions/fitting.html
setwd("C:/Users/joseaugusto/Desktop/clase1/pp/")
estacion=read.csv('estacion9.csv')
setwd("D:/Proyectos_GitHub/curso_hidrologia/data/Estaciones/")
estacion=read.csv('estacion9.csv')
estacion_daily=estacion%>%
mutate(DATE = as.Date(fecha, format="%m/%d/%Y"))
library(lubridate)
library(ggplot2)
library(dplyr)
library(zoo)
estacion
estacion_daily=estacion%>%
mutate(DATE = as.Date(fecha, format="%m/%d/%Y"))
estacion
estacion_daily
names(estacion)
max_mensual=estacion%>%
mutate(fecha = as.Date(fecha, format="%m/%d/%Y"))%>%
mutate(mes = month(DATE),ano=year(DATE))
library(lubridate)
library(ggplot2)
library(dplyr)
library(zoo)
max_mensual=estacion%>%
mutate(fecha = as.Date(fecha, format="%m/%d/%Y"))%>%
mutate(mes = month(fecha),ano=year(fecha))%>%
group_by(ano) %>%
summarise(precip= max(pp))
max_mensual
max_mensual=estacion%>%
mutate(fecha = as.Date(fecha, format="%m/%d/%Y"))%>%
mutate(mes = month(fecha),ano=year(fecha))%>%
group_by(ano)
max_mensual
class(max_mensual)
dim(max_height())
dim(max_mensual)
max_mensual=estacion%>%
mutate(fecha = as.Date(fecha, format="%m/%d/%Y"))%>%
mutate(mes = month(fecha),ano=year(fecha))%>%
group_by(ano) %>%
summarise(precip= max(pp))
dim(max_mensual)
max_mensual$ano
max_mensual$precip
data.frame(max_mensual)
max_mensual=estacion%>%
mutate(fecha = as.Date(fecha, format="%m/%d/%Y"))%>%
mutate(mes = month(fecha),ano=year(fecha))%>%
group_by(ano) %>%
max_mensual=estacion%>%
mutate(fecha = as.Date(fecha, format="%m/%d/%Y"))%>%
mutate(mes = month(fecha),ano=year(fecha))%>%
group_by(ano)
max_mensual
help("summarise")
max_mensual=estacion%>%
mutate(fecha = as.Date(fecha, format="%m/%d/%Y"))%>%
mutate(mes = month(fecha),ano=year(fecha))%>%
group_by(ano) %>%
summarise(precip= max(pp),na.rm = TRUE)
max_mensual
max_mensual=estacion%>%
mutate(fecha = as.Date(fecha, format="%m/%d/%Y"))%>%
mutate(mes = month(fecha),ano=year(fecha))%>%
group_by(ano) %>%
summarise(precip= max(pp,na.rm=T))
max_mensual
